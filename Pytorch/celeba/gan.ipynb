{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "w4aBwJDoRX"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import skimage.io as io\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "keMTMEb2sT"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "root = '/Users/nickeisenberg/GitRepos/DataSets_local/' \n",
        "\n",
        "# training set and train data loader\n",
        "totalset = torchvision.datasets.CelebA(\n",
        "    root=root, split='train', download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(len(totalset) * .8)\n",
        "val_size = len(totalset) - train_size\n",
        "\n",
        "trainset, valset = random_split(totalset, (train_size, val_size))\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    trainset, batch_size=64, shuffle=True\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    valset, batch_size=64, shuffle=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "LNhyjPJAgt"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, 32, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            32, 64, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            64, 128, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            128, 128, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(128 * 4 * 4, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = nn.LeakyReLU(.02)(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.LeakyReLU(.02)(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.LeakyReLU(.02)(x)\n",
        "        x = self.conv4(x)\n",
        "        x = nn.LeakyReLU(.02)(x)\n",
        "        x = self.flatten(x)\n",
        "        x = nn.Dropout(.2)(x)\n",
        "        x = self.linear(x)\n",
        "        des_p_value = nn.Sigmoid()(x)\n",
        "        return des_p_value\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.linear1 = nn.Linear(latent_dim, 16)\n",
        "        self.linear2 = nn.Linear(16, 64)\n",
        "        self.linear3 = nn.Linear(64, 128 * 4 * 4)\n",
        "        self.conv1 = nn.ConvTranspose2d(\n",
        "            128, 128, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.conv2 = nn.ConvTranspose2d(\n",
        "            128, 64, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.conv3 = nn.ConvTranspose2d(\n",
        "            64, 32, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.conv4 = nn.ConvTranspose2d(\n",
        "            32, 3, 4, stride=2, padding=1\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_inputs):\n",
        "        x = self.linear1(latent_inputs)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        x = x.reshape((-1, 128, 4, 4))\n",
        "        x = self.conv1(x)\n",
        "        x = nn.LeakyReLU(.2)(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.LeakyReLU(.2)(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.LeakyReLU(.2)(x)\n",
        "        x = self.conv4(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "X0c1TILNRb"
      },
      "source": [
        "latent_dim = 2\n",
        "discriminator = Discriminator().to(device)\n",
        "generator = Generator(latent_dim).to(device)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "optim_d = torch.optim.Adam(discriminator.parameters(), lr=.0001, )\n",
        "optim_g = torch.optim.Adam(generator.parameters(), lr=.0001)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "9BG1XYDcSq"
      },
      "source": [
        "def train_one_epoch(\n",
        "        train_dataloader=train_dataloader,\n",
        "        latent_dim=2\n",
        "    ):\n",
        "\n",
        "    running_loss_d = 0.\n",
        "    running_loss_g = 0.\n",
        "    \n",
        "    counter = 1\n",
        "    for i, t_data in enumerate(train_dataloader):\n",
        "        counter += 1\n",
        "\n",
        "        t_ims, _ = t_data.to(device)\n",
        "        batch_size = t_ims.shape[0]\n",
        "        \n",
        "        # Discriminator\n",
        "        optim_d.zero_grad()\n",
        "\n",
        "        latents = torch.randn((batch_size, latent_dim)).to(device)\n",
        "        fake_ims = generator(latents).to(device)\n",
        "\n",
        "        combined_ims = torch.vstack((t_ims, fake_ims)).to(device)\n",
        "        combined_labels = torch.hstack(\n",
        "            (\n",
        "                torch.ones(batch_size).to(device), \n",
        "                torch.zeros(batch_size).to(device)\n",
        "            )\n",
        "        ).reshape((-1, 1)).to(device)\n",
        "        combined_labels += .05 * torch.rand((batch_size * 2, 1)).to(device)\n",
        "\n",
        "        d_loss = loss_fn(discriminator(combined_ims), combined_labels)\n",
        "        d_loss.backward()\n",
        "        running_loss_d += d_loss.item()\n",
        "        optim_d.step()\n",
        "        \n",
        "        # Generator \n",
        "        optim_g.zero_grad()\n",
        "\n",
        "        latents = torch.randn((batch_size, latent_dim)).to(device)\n",
        "        fake_ims = generator(latents).to(device)\n",
        "        fake_labels = torch.ones((batch_size, 1)).to(device)\n",
        "\n",
        "        g_loss = loss_fn(discriminator(fake_ims), fake_labels)\n",
        "        g_loss.backward()\n",
        "        running_loss_g += g_loss.item()\n",
        "        optim_g.step()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "            print(f'Batch {i} Disciriminator loss: {running_loss_d / counter}')\n",
        "            print(f'Batch {i} Generator loss: {running_loss_g / counter}')\n",
        "            running_loss_d = 0.\n",
        "            running_loss_g = 0.\n",
        "            counter = 0.\n",
        "\n",
        "    return running_loss_d, running_loss_g\n",
        "\n",
        "\n",
        "def validation_one_epoch(\n",
        "        val_dataloader=val_dataloader,\n",
        "        latent_dim=2\n",
        "    ):\n",
        "\n",
        "    v_running_loss_d = 0.\n",
        "    v_running_loss_g = 0.\n",
        "    counter = 1 \n",
        "    for i, v_data in enumerate(val_dataloader):\n",
        "        counter += 1\n",
        "        v_ims, _ = v_data.to(device)\n",
        "        v_batch_size = v_ims.shape[0]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            latents = torch.randn((v_batch_size, latent_dim)).to(device)\n",
        "            fake_ims = generator(latents).to(device)\n",
        "\n",
        "            combined_ims = torch.vstack((v_ims, fake_ims)).to(device)\n",
        "            combined_labels = torch.hstack(\n",
        "                (\n",
        "                    torch.ones(v_batch_size).to(device),\n",
        "                    torch.zeros(v_batch_size).to(device)\n",
        "                )\n",
        "            ).reshape((-1, 1)).to(device)\n",
        "\n",
        "            v_d_loss = loss_fn(discriminator(combined_ims), combined_labels)\n",
        "            v_running_loss_d += v_d_loss.item()\n",
        "\n",
        "            latents = torch.randn((batch_size, latent_dim)).to(device)\n",
        "            fake_ims = generator(latents).to(device)\n",
        "            fake_labels = torch.ones((batch_size, 1)).to(device)\n",
        "\n",
        "            v_g_loss = loss_fn(discriminator(fake_ims), fake_labels)\n",
        "            v_running_loss_g += v_g_loss.item()\n",
        "\n",
        "    avg_v_loss_d = v_running_loss_d / counter\n",
        "    avg_v_loss_g = v_running_loss_g / counter\n",
        "\n",
        "    return avg_v_loss_d, avg_v_loss_g"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "J9kMZL0mb0"
      },
      "source": [
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch: {epoch}')\n",
        "    _ = discriminator.train()\n",
        "    _ = generator.train()\n",
        "    avg_d_loss, avg_g_loss = train_one_epoch()\n",
        "\n",
        "    _ = discriminator.eval()\n",
        "    _ = generator.eval()\n",
        "    avg_v_loss_d, avg_v_loss_g = validation_one_epoch()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}